# ğŸ“ Lia AI Service

Sistema de IA para o App de Estudos construÃ­do em cima de **LangGraph + FastAPI**.

## ğŸš€ VisÃ£o Geral

Um backend Python assÃ­ncrono que orquestra agentes educacionais, geraÃ§Ã£o de conteÃºdo
personalizado (flashcards, quizzes, planos de estudo) e integraÃ§Ãµes com OpenAI e Supabase.

## ğŸ§  Principais Tecnologias
- **FastAPI** para APIs assÃ­ncronas e escalÃ¡veis
- **LangGraph** para orquestraÃ§Ã£o de agentes e fluxos multi-etapa
- **OpenAI (GPT-4.x / GPT-5-nano)** via clientes assÃ­ncronos com controle de concorrÃªncia
- **Supabase/PostgreSQL** para persistÃªncia (conversas, perfis, memÃ³rias)
- **AsyncIO** + limitadores de taxa para suportar centenas de requisiÃ§Ãµes simultÃ¢neas

## âš™ï¸ Funcionalidades

### ğŸ¤– Chat com Lia
- Assistente de estudos personalizada com memÃ³ria de conversaÃ§Ã£o
- Perfil do usuÃ¡rio influencia tom, nÃ­vel de detalhe e exemplos
- Fallback seguro quando integraÃ§Ãµes externas estÃ£o indisponÃ­veis

### ğŸ“š GeraÃ§Ã£o de ConteÃºdo
- **Flashcards multi-agente** com monitoramento em tempo real (SSE)
- **Quizzes** e **notas estruturadas** usando ferramentas LangGraph
- **Planos de estudo** personalizados e conversas guiadas

### ğŸ”’ Confiabilidade e Escalabilidade
- Clientes OpenAI centralizados (`openai_utils`) com semÃ¡foro (`OPENAI_CONCURRENCY_LIMIT`)
- GeraÃ§Ã£o paralela via `asyncio.gather`, sem `ThreadPoolExecutor`
- Progressos armazenados em cache e expurgados apÃ³s conclusÃ£o
- Logs estruturados (`logger`) para todas as etapas crÃ­ticas

## ğŸ“¦ InstalaÃ§Ã£o

### PrÃ©-requisitos
- Python 3.10+
- OpenAI API Key
- Supabase Service Role Key (para persistÃªncia opcional)

### Setup rÃ¡pido
```bash
cd lia-ai-service
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
cp .env.example .env  # configure suas credenciais
```

### VariÃ¡veis de ambiente essenciais
```env
# OpenAI
OPENAI_API_KEY=sk-...
OPENAI_DEFAULT_MODEL=gpt-5-nano
OPENAI_CONCURRENCY_LIMIT=8  # chamadas simultÃ¢neas ao OpenAI

# Supabase (p/ persistÃªncia)
SUPABASE_URL=https://<project>.supabase.co
SUPABASE_SERVICE_ROLE_KEY=...

# FastAPI
HOST=0.0.0.0
PORT=8000
DEBUG=true
```

## ğŸƒâ€â™‚ï¸ Executando

### Desenvolvimento
```bash
source venv/bin/activate
uvicorn main:app --reload --host 0.0.0.0 --port 8000
```

### ProduÃ§Ã£o
```bash
# Uvicorn standalone
uvicorn main:app --host 0.0.0.0 --port 8000

# Gunicorn + workers assÃ­ncronos
gunicorn main:app -w 4 -k uvicorn.workers.UvicornWorker --bind 0.0.0.0:8000
```

### Docker
```dockerfile
FROM python:3.11-slim
WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt
COPY . .
ENV HOST=0.0.0.0 PORT=8000
EXPOSE 8000
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
```

## ğŸ“– Endpoints principais

| MÃ©todo | Rota                              | DescriÃ§Ã£o                                  |
|--------|-----------------------------------|---------------------------------------------|
| GET    | `/`                               | Status bÃ¡sico                               |
| GET    | `/health`                         | Health-check detalhado                      |
| POST   | `/chat`                           | Chat simples com a Lia                      |
| POST   | `/chat/advanced`                  | Chat com fluxo LangGraph                    |
| POST   | `/generate-flashcards`            | Flashcards multi-agente (SSE em `/progress`) |
| POST   | `/generate-flashcards/from-text`  | Flashcards a partir de texto livre          |
| POST   | `/generate-quiz`                 | Quizzes personalizados                      |
| POST   | `/generate-notes`                | Notas/resumos estruturados                  |
| POST   | `/study-plan/create`             | Plano de estudo personalizado               |

### Exemplo de requisiÃ§Ã£o (`POST /chat`)
```json
{
  "message": "Oi Lia! Pode me explicar fotossÃ­ntese?",
  "conversation_id": "conv_123",
  "user_id": "user_456"
}
```

### Monitoramento de progresso SSE
- `GET /progress/{operation_id}` retorna eventos `queued`, `processing`, `completed` com percentuais.

## ğŸ§± Arquitetura em alto nÃ­vel

```
src/
â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ lia_agent.py                 # Agente LangGraph principal (memÃ³rias, ferramentas)
â”‚   â””â”€â”€ multi_agent_flashcards.py    # GeraÃ§Ã£o paralela de flashcards (asyncio)
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ requests.py
â”‚   â””â”€â”€ responses.py
â”œâ”€â”€ routers/
â”‚   â”œâ”€â”€ chat.py
â”‚   â”œâ”€â”€ content_generation.py
â”‚   â””â”€â”€ health.py
â””â”€â”€ services/
    â”œâ”€â”€ ai_service.py                # Orquestra lÃ³gica de IA e progress tracking
    â”œâ”€â”€ database_service.py          # Supabase (conversas, perfis)
    â””â”€â”€ openai_utils.py              # Wrapper assÃ­ncrono + semÃ¡foro OpenAI
```

- `openai_utils.py`: centraliza clientes `AsyncOpenAI` e impÃµe limite de concorrÃªncia (`OPENAI_CONCURRENCY_LIMIT`).
- `ai_service.py`: expÃµe endpoints + controla SSE, caching de conversas e fallbacks.
- `multi_agent_flashcards.py`: usa `asyncio.gather` para dividir lotes e reportar progresso.

## ğŸ”— IntegraÃ§Ã£o com o app Expo/React Native

- RequisiÃ§Ãµes locais passam por `/api/lia/*` (proxy do Metro/Expo) ou proxy Express (`LIA_PROXY_PORT`).
- Ajuste as variÃ¡veis `EXPO_PUBLIC_LIA_SERVICE_DEV_URL`, `LIA_AI_SERVICE_DEV_URL` e `OPENAI_CONCURRENCY_LIMIT` no `.env` compartilhado.
- SSE (Server-Sent Events) sÃ£o consumidos com `EventSource` para exibir andamento de geraÃ§Ãµes longas.

## ğŸ¤ ContribuiÃ§Ã£o e testes

1. Crie um fork / branch
2. Rode lint/testes (ex.: `pytest`, `ruff`, `mypy` se configurado)
3. Descreva na PR alteraÃ§Ãµes e cenÃ¡rios testados

### Testes sugeridos
- `pytest` (unit e integraÃ§Ãµes futuras)
- ExercÃ­cios manuais: `POST /generate-flashcards` e `GET /progress/{id}` para validar SSE
- Monitorar logs (`uvicorn --reload`) verificando concorrÃªncia e limites OpenAI

## ğŸ“Š Monitoramento e OperaÃ§Ã£o

- **Health-check** automÃ¡tico via `/health`
- Logs estruturados (nÃ­veis: info, warning, error)
- MÃ©tricas de progresso em `progress_store` e Supabase (quando disponÃ­vel)
- Ajuste `OPENAI_CONCURRENCY_LIMIT` conforme limites do plano OpenAI para evitar erros 429
